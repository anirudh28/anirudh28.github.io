---
title: "From BERT to Mamba: Evaluating Deep Learning for Efficient QA Systems"
excerpt: "This work explores the trade-offs between accuracy and computational efficiency in question-answering (QA) systems comprising of LLMs<br/><img src='/images/QA.png'>"
collection: portfolio
---

This project aims to compare the performance of multiple deep learning models for a Question Answering NLP task, focusing on balancing accuracy and computational efficiency. Effective question-answering requires models to interpret queries precisely within a given context while remaining resource-efficient
for real-world applications. We fine-tune, and evaluate various deep learning models such as BERT, T5, and Mamba, assessing their Exact Match scores and resource usage. Through this comparative analysis, we explore the
trade-offs that shape each modelâ€™s performance, providing insights into optimizing QA systems for practical deployment. 
[PPT](https://github.com/anirudh28/From-BERT-to-Mamba-Evaluating-Deep-Learning-for-Efficient-QA-Systems/blob/main/Project_PPT.pdf) [Code](https://github.com/anirudh28/From-BERT-to-Mamba-Evaluating-Deep-Learning-for-Efficient-QA-Systems/tree/main) [Report](https://github.com/anirudh28/From-BERT-to-Mamba-Evaluating-Deep-Learning-for-Efficient-QA-Systems/blob/main/Final_Report.pdf) 
